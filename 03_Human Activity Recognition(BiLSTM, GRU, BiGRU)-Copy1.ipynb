{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c819605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU, Bidirectional\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Flatten\n",
    "\n",
    "# 데이터 디렉토리 설정\n",
    "data_dir = \"C:/Users/wdd45/OneDrive/바탕 화면/딥러닝응용/human/UCI HAR Dataset/UCI HAR Dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84b34b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩 및 전처리\n",
    "def load_HAR_data(data_dir):\n",
    "    train_signals, train_labels, test_signals, test_labels = [], [], [], []\n",
    "    \n",
    "    for signal_type in [\"train\", \"test\"]:\n",
    "        for signal in [\"Inertial Signals/total_acc_x_\", \"Inertial Signals/body_acc_x_\", \"Inertial Signals/body_gyro_x_\"]:\n",
    "            filename = f\"{data_dir}/{signal_type}/{signal}{signal_type}.txt\"\n",
    "            data = pd.read_csv(filename, delim_whitespace=True, header=None).values\n",
    "            if \"train\" in signal_type:\n",
    "                train_signals.append(data)\n",
    "            else:\n",
    "                test_signals.append(data)\n",
    "\n",
    "        labels_filename = f\"{data_dir}/{signal_type}/y_{signal_type}.txt\"\n",
    "        labels = pd.read_csv(labels_filename, header=None, names=[\"Activity\"]).values\n",
    "        if \"train\" in signal_type:\n",
    "            train_labels.append(labels)\n",
    "        else:\n",
    "            test_labels.append(labels)\n",
    "\n",
    "    train_signals = np.transpose(np.array(train_signals), (1, 2, 0))\n",
    "    train_labels = train_labels[0]\n",
    "    test_signals = np.transpose(np.array(test_signals), (1, 2, 0))\n",
    "    test_labels = test_labels[0]\n",
    "\n",
    "    train_labels = train_labels - 1\n",
    "    test_labels = test_labels - 1\n",
    "\n",
    "    train_signals = (train_signals - np.mean(train_signals)) / np.std(train_signals)\n",
    "    test_signals = (test_signals - np.mean(test_signals)) / np.std(test_signals)\n",
    "\n",
    "    return train_signals, train_labels, test_signals, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00fa1dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_signals, train_labels, test_signals, test_labels = load_HAR_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea70dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 훈련 및 검증 세트로 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_signals, train_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d5e92fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot 인코딩\n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5db6cc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_2 (Bidirectio  (None, 128, 128)         34816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16384)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               2097280   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,132,870\n",
      "Trainable params: 2,132,870\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002684ECD9708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002684ECD9708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.7113 - accuracy: 0.6829WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000026850993E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000026850993E58> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "184/184 [==============================] - 22s 101ms/step - loss: 0.7113 - accuracy: 0.6829 - val_loss: 0.4925 - val_accuracy: 0.7553\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 18s 96ms/step - loss: 0.3637 - accuracy: 0.8191 - val_loss: 0.3736 - val_accuracy: 0.8022\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 18s 98ms/step - loss: 0.3236 - accuracy: 0.8312 - val_loss: 0.3776 - val_accuracy: 0.8042\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 17s 95ms/step - loss: 0.2927 - accuracy: 0.8449 - val_loss: 0.3508 - val_accuracy: 0.8205\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 18s 95ms/step - loss: 0.2607 - accuracy: 0.8604 - val_loss: 0.2926 - val_accuracy: 0.8477\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 16s 86ms/step - loss: 0.2276 - accuracy: 0.8737 - val_loss: 0.2867 - val_accuracy: 0.8382\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 16s 86ms/step - loss: 0.2446 - accuracy: 0.8623 - val_loss: 0.2824 - val_accuracy: 0.8470\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 16s 86ms/step - loss: 0.2096 - accuracy: 0.8851 - val_loss: 0.2752 - val_accuracy: 0.8613\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 16s 86ms/step - loss: 0.2187 - accuracy: 0.8822 - val_loss: 0.2880 - val_accuracy: 0.8430\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 16s 86ms/step - loss: 0.1990 - accuracy: 0.8854 - val_loss: 0.2514 - val_accuracy: 0.8824\n",
      "93/93 [==============================] - 2s 19ms/step - loss: 0.3501 - accuracy: 0.8795\n",
      "BiLSTM Test accuracy: 87.95%\n"
     ]
    }
   ],
   "source": [
    "# BiLSTM 모델\n",
    "bi_lstm_model = Sequential()\n",
    "bi_lstm_model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "bi_lstm_model.add(Flatten())\n",
    "bi_lstm_model.add(Dense(128, activation='relu'))\n",
    "bi_lstm_model.add(Dense(6, activation='softmax'))\n",
    "bi_lstm_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "bi_lstm_model.summary()\n",
    "\n",
    "bi_lstm_history = bi_lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "bi_lstm_test_loss, bi_lstm_test_accuracy = bi_lstm_model.evaluate(test_signals, test_labels)\n",
    "print(f\"BiLSTM Test accuracy: {bi_lstm_test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6792f2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_3 (GRU)                 (None, 64)                13248     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,342\n",
      "Trainable params: 22,342\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000026850C824C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000026850C824C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "184/184 [==============================] - ETA: 0s - loss: 1.1390 - accuracy: 0.5031WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002684FC29AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002684FC29AF8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "184/184 [==============================] - 11s 47ms/step - loss: 1.1390 - accuracy: 0.5031 - val_loss: 0.7969 - val_accuracy: 0.6173\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 8s 41ms/step - loss: 0.6190 - accuracy: 0.6904 - val_loss: 0.5396 - val_accuracy: 0.6988\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 7s 40ms/step - loss: 0.4715 - accuracy: 0.7604 - val_loss: 0.4756 - val_accuracy: 0.7458\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 7s 40ms/step - loss: 0.4093 - accuracy: 0.7987 - val_loss: 0.4141 - val_accuracy: 0.8001\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 7s 40ms/step - loss: 0.3600 - accuracy: 0.8278 - val_loss: 0.4110 - val_accuracy: 0.7974\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 7s 40ms/step - loss: 0.3288 - accuracy: 0.8352 - val_loss: 0.3346 - val_accuracy: 0.8294\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 7s 40ms/step - loss: 0.3069 - accuracy: 0.8442 - val_loss: 0.3857 - val_accuracy: 0.8076\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 7s 39ms/step - loss: 0.2981 - accuracy: 0.8448 - val_loss: 0.3086 - val_accuracy: 0.8423\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 7s 40ms/step - loss: 0.2765 - accuracy: 0.8573 - val_loss: 0.3392 - val_accuracy: 0.8321\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 7s 40ms/step - loss: 0.2720 - accuracy: 0.8589 - val_loss: 0.2890 - val_accuracy: 0.8484\n",
      "93/93 [==============================] - 1s 12ms/step - loss: 0.4571 - accuracy: 0.8185\n",
      "GRU Test accuracy: 81.85%\n"
     ]
    }
   ],
   "source": [
    "# GRU 모델\n",
    "gru_model = Sequential()\n",
    "gru_model.add(GRU(64, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "gru_model.add(Dense(128, activation='relu'))\n",
    "gru_model.add(Dense(6, activation='softmax'))\n",
    "gru_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 모델 빌드\n",
    "gru_model.build((None, X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "gru_model.summary()\n",
    "\n",
    "gru_history = gru_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "gru_test_loss, gru_test_accuracy = gru_model.evaluate(test_signals, test_labels)\n",
    "print(f\"GRU Test accuracy: {gru_test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3e175bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_4 (Bidirectio  (None, 128, 128)         26496     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 16384)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               2097280   \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 6)                 774       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,124,550\n",
      "Trainable params: 2,124,550\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002684FC9F048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000002684FC9F048> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "184/184 [==============================] - ETA: 0s - loss: 0.8492 - accuracy: 0.6523WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002685892DCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x000002685892DCA8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "184/184 [==============================] - 19s 88ms/step - loss: 0.8492 - accuracy: 0.6523 - val_loss: 0.4968 - val_accuracy: 0.7736\n",
      "Epoch 2/10\n",
      "184/184 [==============================] - 15s 84ms/step - loss: 0.4003 - accuracy: 0.8070 - val_loss: 0.3679 - val_accuracy: 0.8239\n",
      "Epoch 3/10\n",
      "184/184 [==============================] - 16s 85ms/step - loss: 0.3108 - accuracy: 0.8424 - val_loss: 0.3566 - val_accuracy: 0.8042\n",
      "Epoch 4/10\n",
      "184/184 [==============================] - 16s 85ms/step - loss: 0.2775 - accuracy: 0.8493 - val_loss: 0.3170 - val_accuracy: 0.8083\n",
      "Epoch 5/10\n",
      "184/184 [==============================] - 15s 84ms/step - loss: 0.2520 - accuracy: 0.8590 - val_loss: 0.3076 - val_accuracy: 0.8423\n",
      "Epoch 6/10\n",
      "184/184 [==============================] - 16s 84ms/step - loss: 0.2470 - accuracy: 0.8640 - val_loss: 0.2706 - val_accuracy: 0.8504\n",
      "Epoch 7/10\n",
      "184/184 [==============================] - 15s 84ms/step - loss: 0.2236 - accuracy: 0.8733 - val_loss: 0.2879 - val_accuracy: 0.8552\n",
      "Epoch 8/10\n",
      "184/184 [==============================] - 15s 84ms/step - loss: 0.2416 - accuracy: 0.8718 - val_loss: 0.2718 - val_accuracy: 0.8477\n",
      "Epoch 9/10\n",
      "184/184 [==============================] - 15s 84ms/step - loss: 0.2070 - accuracy: 0.8793 - val_loss: 0.3100 - val_accuracy: 0.8443\n",
      "Epoch 10/10\n",
      "184/184 [==============================] - 16s 86ms/step - loss: 0.2164 - accuracy: 0.8847 - val_loss: 0.2726 - val_accuracy: 0.8511\n",
      "93/93 [==============================] - 2s 16ms/step - loss: 0.3953 - accuracy: 0.8358\n",
      "BiGRU Test accuracy: 83.58%\n"
     ]
    }
   ],
   "source": [
    "# BiGRU 모델\n",
    "bi_gru_model = Sequential()\n",
    "bi_gru_model.add(Bidirectional(GRU(64, return_sequences=True), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "bi_gru_model.add(Flatten())\n",
    "bi_gru_model.add(Dense(128, activation='relu'))\n",
    "bi_gru_model.add(Dense(6, activation='softmax'))\n",
    "bi_gru_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "bi_gru_model.summary()\n",
    "\n",
    "bi_gru_history = bi_gru_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "bi_gru_test_loss, bi_gru_test_accuracy = bi_gru_model.evaluate(test_signals, test_labels)\n",
    "print(f\"BiGRU Test accuracy: {bi_gru_test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3b3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
